{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Based Approach",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW4rRh1bgo9w"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJet1iLkQ1nG"
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJcl7Ok7hUp4"
      },
      "source": [
        "data = pd.read_csv('/content/FinalDataset.csv')\n",
        "data.TWEET=data.TWEET.astype(str)\n",
        "\n",
        "#Transform text to lowercase\n",
        "data['TWEET'].apply(lambda x: x.lower()) \n",
        "#Removing all punctuations and special characters\n",
        "data['TWEET'] = data['TWEET'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)) \n",
        "#Removing all repeated characters\n",
        "#data['TWEET'] = data['TWEET'].apply(lambda x: re.sub(r'(.)\\1+', r'\\1\\1', '', x)) "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mENa1wMY88my"
      },
      "source": [
        "StopWord Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByoRzO1E-b2t"
      },
      "source": [
        "english_stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "hindi_stopwords = ['ke', 'ka', 'ek', 'mein', 'ki', 'hai', 'yah', 'aur', 'se', 'hain', 'ko', 'par', 'iss', 'hota', 'jo', 'kar', 'me', 'gaya', 'karne', 'kiya', 'liye', 'apne', 'ne', 'bani', 'nahi', 'toh', 'hi', 'ya', 'avam', 'diya', 'ho', 'iska', 'tha', 'dhvara', 'hua', 'tak', 'saath', 'karna', 'vaale', 'baad', 'liya', 'aap', 'kuchh', 'sakte', 'kisi', 'iska', 'sabse', 'ismein', 'the', 'do', 'hone', 'vah', 've', 'karte', 'bahut', 'kaha', 'varg', 'kai', 'karein', 'hoti', 'apni', 'unke', 'thi', 'yadi', 'hui', 'jaa', 'na', 'ise', 'kehte', 'kahte', 'jab', 'hote', 'koi', 'hue', 'va', 'abhi', 'jaise', 'sabhi', 'karta', 'unki', 'tarah', 'uss', 'aadi', 'kul', 'raha', 'iski', 'sakta', 'rahe', 'unka', 'issi', 'rakhein', 'apna', 'pe', 'uske', 'kyu', 'kyun', 'modi', 'isko', 'iska', 'tum', 'tumhara', 'kyon', 'kyunki', 'log', 'bhai', 'kya', 'teri', 'tere', 'tera', 'ab', 'ye', 'yeh', 'ekto', 'aapko', 'ha', 'haan', 'aapka', 'yahi', 'meri', 'mera', 'h', 'u', 'k', 'pr', 'jise', 'jis', 'jisko', 'ham', 'hum', 'hamara', 'humara', 'hamari', 'humari', 'jitna', 'jitne', 'ur', 'wo', 'woh', 'waha', 'wahan', 'vaha', 'vahan', 'kya', 'koh', 'tumne', 'tumn', 'raha', 'rha', 'raho', 'kahun', 'kahu', 'kahan', 'kaha', 'jesi', 'jaisa', 'jaise', 'kitna', 'kitne', 'kitn', 'kitni', 'wale', 'wala', 'didi', 'bhi', 'phir', 'vapas', 'hei', 'hein', 'bana', 'sakte', 'ji', 'n', 'b', 'kisko', 'kisiko', 'apni', 'apna', 'apne', 'hote', 'hota', 'baad', 'mei', 'p', 'ap', 'oye', 'hoye', 'rahe', 'rahi', 'sab', 'apn', 'yhi', 'tmko', 'hu', 'kyonki', 'mere', 'g', 'apka', 'apke', 'apki', 'aapki', 'aapke', 'kaun', 'kab', 'kabhi', 'mai', 'dia', 'lia', 'baat', 'bat', 'hona', 'honi', 'chahiye', 'chahie', 'din', 'saal', 'samay', 'lekin', 'unhe', 'iske', 'gaye', 'gayi']\n",
        "stopwords_list = english_stopwords + hindi_stopwords\n",
        "data['TWEET'] = data['TWEET'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "DltOvRJyhhyb",
        "outputId": "ab5f2ad7-9bf2-423d-8816-ab9d160874eb"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>TWEET</th>\n",
              "      <th>SCORE</th>\n",
              "      <th>SENTIMENT_LABEL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>pakistan ghra tauq pakistan israel tasleem nah...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>madarchod mull mathura dikha mullo hindu liy m...</td>\n",
              "      <td>0</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>manya pradhan mantri mahoday shriman narendra ...</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>krishna jcb full trend chal aa nan</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>loksabha janta sirf vote de mp bjp without</td>\n",
              "      <td>2</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ... SENTIMENT_LABEL\n",
              "0           0  ...        negative\n",
              "1           1  ...        negative\n",
              "2           2  ...        positive\n",
              "3           3  ...        positive\n",
              "4           4  ...        positive\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rRWxaDXh7d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a25344e-2937-46e2-d10c-017499c72f68"
      },
      "source": [
        "count = 0\n",
        "for index in data.index:\n",
        "  if data['SCORE'][index] > 0:\n",
        "    count += 1\n",
        "print(count)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10342\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOn8Ph_zkvCn"
      },
      "source": [
        "X = data.iloc[:, 1]\n",
        "y = data.iloc[:, -1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPeCofelkLDx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd8NBZwllXpA",
        "outputId": "d18c91a9-b348-42a8-e689-89926bcc2a43"
      },
      "source": [
        "X_train"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9570             wow pata hf open g3 achi vese hf seri gol\n",
              "11879       think acju provid servic free women app imagin\n",
              "10598       mla pleas follow advic given lokpriya neta nan\n",
              "6916     best solut end terror kashmir 1 terrorist marn...\n",
              "10672        badhta badhn time sasta isi chakkar desh daal\n",
              "                               ...                        \n",
              "5191     nowplay best abba dodworth collieri mw brass band\n",
              "13418    jaiveeer aisa chutiy koun bolta separ south in...\n",
              "5390     logun gallay kaat deni chahy pashtun naam begh...\n",
              "860      jay jay shree ram ram rajy aaya danavo rakshas...\n",
              "7270          har tensn jad dwa sandeep maheswari show nan\n",
              "Name: TWEET, Length: 10945, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpXiSy-HWNpl",
        "outputId": "543c6b7e-4f54-42b8-e0ef-01880c3b5a53"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9698     saver best place get kid toy got switch go din...\n",
              "14293    richestbil gateszuckerbergmukesh ambani bollyw...\n",
              "907      chutiy stoner nahi hu na teri tarh kuch sungn ...\n",
              "5205                       teri khair fair bemari shame u \n",
              "3482     pooja hehe rahul gandhi ki fati hui langot hon...\n",
              "                               ...                        \n",
              "10632    aay haay gaurimaa dulhan se kam nahi chamak ra...\n",
              "13378    bhai wondin nhi bhoola jab 2003 ka world cup h...\n",
              "2517                happi birthday rahn sinclair  wolfsban\n",
              "2221     aap ne bhut sare apn realm fan kho diy sir men...\n",
              "8231     tum daikhna ye waqt zaror badley ga wo kese ya...\n",
              "Name: TWEET, Length: 3649, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64Jpb5X89J_U"
      },
      "source": [
        "TfidVectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6qToO8bkAeC"
      },
      "source": [
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "my_stop_words = text.ENGLISH_STOP_WORDS\n",
        "# Create feature vectors\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.95,\n",
        "                             stop_words=my_stop_words,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True,\n",
        "                             ngram_range=(2,2))\n",
        "train_vectors = vectorizer.fit_transform(X_train.values.astype('U'))\n",
        "test_vectors = vectorizer.transform(X_test.values.astype('U'))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_iRZGGK9Ow_"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zMpSQxHmghi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef0b8d3-863b-4fa6-815d-2dc6b519bc02"
      },
      "source": [
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "# Perform classification with SVM, kernel=linear\n",
        "classifier = RandomForestClassifier()\n",
        "t0 = time.time()\n",
        "classifier.fit(train_vectors, y_train)\n",
        "t1 = time.time()\n",
        "prediction = classifier.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(y_test, prediction, output_dict=True,labels=np.unique(prediction))\n",
        "print('positive: ', report['positive'])\n",
        "print('negative: ', report['negative'])\n",
        "print('neutral: ', report['neutral'])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 11.949771s; Prediction time: 0.337605s\n",
            "positive:  {'precision': 0.6419951729686243, 'recall': 0.6627906976744186, 'f1-score': 0.6522272170004086, 'support': 1204}\n",
            "negative:  {'precision': 0.6347914547304171, 'recall': 0.5942857142857143, 'f1-score': 0.6138711264141662, 'support': 1050}\n",
            "neutral:  {'precision': 0.534082923401265, 'recall': 0.5448028673835126, 'f1-score': 0.539389638041164, 'support': 1395}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6e7a6Dk9UCH"
      },
      "source": [
        "SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S0uPd6afwI7",
        "outputId": "0526a9d5-ab4a-41f7-c99d-2ec0d65e7460"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "classifier = SVC(kernel = \"rbf\")\n",
        "t0 = time.time()\n",
        "classifier.fit(train_vectors, y_train)\n",
        "t1 = time.time()\n",
        "prediction = classifier.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(y_test, prediction, output_dict=True)\n",
        "print('positive: ', report['positive'])\n",
        "print('negative: ', report['negative'])\n",
        "print('neutral: ', report['neutral'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 16.176422s; Prediction time: 3.328795s\n",
            "positive:  {'precision': 0.6793960923623446, 'recall': 0.6353820598006644, 'f1-score': 0.6566523605150215, 'support': 1204}\n",
            "negative:  {'precision': 0.6494505494505495, 'recall': 0.5628571428571428, 'f1-score': 0.6030612244897959, 'support': 1050}\n",
            "neutral:  {'precision': 0.527588344699318, 'recall': 0.6100358422939068, 'f1-score': 0.5658244680851064, 'support': 1395}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spyaukBA9WBF"
      },
      "source": [
        "K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZtS4wkUfNzI",
        "outputId": "7872b6dd-ba2b-4f4e-8f90-1f44a2b4f99d"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier()\n",
        "t0 = time.time()\n",
        "classifier.fit(train_vectors, y_train)\n",
        "t1 = time.time()\n",
        "prediction = classifier.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(y_test, prediction, output_dict=True)\n",
        "print('positive: ', report['positive'])\n",
        "print('negative: ', report['negative'])\n",
        "print('neutral: ', report['neutral'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 0.020059s; Prediction time: 0.796348s\n",
            "positive:  {'precision': 0.689119170984456, 'recall': 0.11046511627906977, 'f1-score': 0.19040801717967074, 'support': 1204}\n",
            "negative:  {'precision': 0.29464285714285715, 'recall': 0.06285714285714286, 'f1-score': 0.10361067503924648, 'support': 1050}\n",
            "neutral:  {'precision': 0.3895420792079208, 'recall': 0.9025089605734767, 'f1-score': 0.5441971039550464, 'support': 1395}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JK92YVF_9aYv"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHJ6vuwPfgQX",
        "outputId": "81d223f0-c26a-47d7-c295-d57e841e0f7c"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "t0 = time.time()\n",
        "classifier.fit(train_vectors, y_train)\n",
        "t1 = time.time()\n",
        "prediction = classifier.predict(test_vectors)\n",
        "t2 = time.time()\n",
        "time_linear_train = t1-t0\n",
        "time_linear_predict = t2-t1\n",
        "# results\n",
        "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
        "report = classification_report(y_test, prediction, output_dict=True)\n",
        "print('positive: ', report['positive'])\n",
        "print('negative: ', report['negative'])\n",
        "print('neutral: ', report['neutral'])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training time: 1.308794s; Prediction time: 0.000871s\n",
            "positive:  {'precision': 0.6577851790174855, 'recall': 0.6561461794019934, 'f1-score': 0.656964656964657, 'support': 1204}\n",
            "negative:  {'precision': 0.6345177664974619, 'recall': 0.5952380952380952, 'f1-score': 0.6142506142506143, 'support': 1050}\n",
            "neutral:  {'precision': 0.5311004784688995, 'recall': 0.556989247311828, 'f1-score': 0.5437368789363191, 'support': 1395}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}